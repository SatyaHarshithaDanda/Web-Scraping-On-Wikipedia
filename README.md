# 🌐 Web Scraping on Wikipedia

This project demonstrates how to perform **web scraping on Wikipedia** using Python to extract structured data from unstructured HTML content. It focuses on collecting tabular and textual information from Wikipedia pages and transforming it into a clean, analyzable format.

## 🎯 Objective

The primary goals of the project are:
- Learn and implement web scraping techniques using `requests` and `BeautifulSoup`.
- Extract tabular or text-based data from Wikipedia.
- Convert scraped data into a structured format (like a Pandas DataFrame).
- Perform basic cleaning and analysis on the extracted data.

## 🧰 Tools & Technologies Used

- **Python**
- **BeautifulSoup** – For parsing HTML
- **Requests** – For sending HTTP requests
- **Pandas** – For storing and analyzing scraped data
- **Jupyter Notebook** – For writing and executing code interactively

## 🕸️ Website Targeted

- [Wikipedia.org](https://www.wikipedia.org/)  
  Wikipedia offers a rich source of structured and semi-structured content, ideal for scraping use-cases like tables, infoboxes, lists, etc.

## 🗂️ Data Extracted

The notebook may include extraction of:
- Tables (e.g., country statistics, population, literacy rates, sports data, etc.)
- Lists or paragraphs
- Hyperlinks or references (optional)

## 📊 Key Features

- Extracting and parsing HTML content
- Locating and processing specific HTML tags (tables, lists, divs, spans, etc.)
- Converting web data to structured DataFrames
- Cleaning and storing data in CSV format

## 🧠 Skills Demonstrated

- Web Scraping
- HTML parsing and data extraction
- Data Cleaning
- Exploratory Data Analysis (optional)

## ▶️ How to Run

1. Clone the repository or download the notebook.
2. Install the required packages:
   ```bash
   pip install requests beautifulsoup4 pandas
3. Open and run Web_Scraping_On_Wikipedia_.ipynb in Jupyter Notebook or compatible IDE.

## ⚠️ Disclaimer

Please use scraped data ethically and ensure compliance with Wikipedia’s Terms of Use.
